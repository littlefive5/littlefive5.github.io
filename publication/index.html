<!DOCTYPE html>
<html lang="en-us"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="content-type" content="text/html">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title itemprop="name">Publication | Yunzhi Yao</title>
<meta property="og:title" content="Publication | Yunzhi Yao" />
<meta name="twitter:title" content="Publication | Yunzhi Yao" />
<meta itemprop="name" content="Publication | Yunzhi Yao" />
<meta name="application-name" content="Publication | Yunzhi Yao" />
<meta property="og:site_name" content="Yunzhi Yao" />

<meta name="description" content="Minimal Hugo blog theme with light and dark mode support">
<meta itemprop="description" content="Minimal Hugo blog theme with light and dark mode support" />
<meta property="og:description" content="Minimal Hugo blog theme with light and dark mode support" />
<meta name="twitter:description" content="Minimal Hugo blog theme with light and dark mode support" />

<meta property="og:locale" content="en-us" />
<meta name="language" content="en-us" />

  <link rel="alternate" hreflang="en-us" href="http://localhost:1313/publication/" title="" />






<meta name="generator" content="Hugo 0.143.1">

    
    <meta property="og:url" content="http://localhost:1313/publication/">
  <meta property="og:site_name" content="Yunzhi Yao">
  <meta property="og:title" content="Publication">
  <meta property="og:description" content="(*: Equal contribution). See full list in Google Scholar
2025 Reflection on Knowledge Editing: Charting the Next Steps. Yunzhi Yao, Canyu Chen, Jia-Chen Gu, Shumin Deng, Manling Li, Nanyun Peng. Blog CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners. Yunzhi Yao, Jizhan Fang, Jia-Chen Gu, Ningyu Zhang, Shumin Deng, Huajun Chen, Nanyun Peng. PDF Code How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training. Yixin Ou, Yunzhi Yao, Ningyu Zhang, Hui Jin, Jiacheng Sun, Shumin Deng, Zhenguo Li, Huajun Chen. PDF Code 2024 Knowledge Circuits in Pretrained Transformers. Yunzhi Yao, Ningyu Zhang, Zekun Xi, Mengru Wang, Ziwen Xu, Shumin Deng, Huajun Chen. In Proceedings of the 38th Neural Information Processing Systems (Neurips 2024) PDF Code Video Exploring Model Kinship for Merging Large Language Models. Yedi Hu, Yunzhi Yao, Ningyu Zhang, Shumin Deng, Huajun Chen. PDF Code Benchmarking Chinese Knowledge Rectification in Large Language Models. Jizhan Fang*, Tianhe Lu*, Yunzhi Yao*, Xin Xu, Ningyu Zhang, Huajun Chen. PDF Code Knowledge mechanisms in large language models: A survey and perspective. Mengru Wang*, Yunzhi Yao*, Ziwen Xu, Shuofei Qiao, Shumin Deng, Peng Wang, Xiang Chen, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen. In Findings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP 2024) PDF A comprehensive study of knowledge editing for large language models. Ningyu Zhang*, Yunzhi Yao*, Bozhong Tian*, Peng Wang*, Shumin Deng*, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, Huajun Chen. PDF Post 2023 Unveiling the pitfalls of knowledge editing for large language models. Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen. In Proceedings of the 12th International Conference on Learning Representations (ICLR 2023) PDF Code Editing Large Language Models: Problems, Methods, and Opportunities. Yunzhi Yao*, Peng Wang*, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, Ningyu Zhang. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023) PDF Code Video Knowledge Rumination for Pre-trained Language Models. Yunzhi Yao, Peng Wang, Shengyu Mao, Chuanqi Tan, Fei Huang, Huajun Chen, Ningyu Zhang. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023) PDF Code Schema-aware reference as prompt improves data-efficient knowledge graph construction. Yunzhi Yao*, Shengyu Mao*, Ningyu Zhang, Xiang Chen, Shumin Deng, Xi Chen, Huajun Chen. In Proceedings of the 46th International ACM SIGIR Conference (SIGIR 2023) PDF Code Before 2022 Kformer: Knowledge injection in transformer feed-forward layers. Yunzhi Yao, Shaohan Huang, Li Dong, Furu Wei, Huajun Chen, Ningyu Zhang. In Proceedings of the 11th Natural Language Processing and Chinese Computing (NLPCC 2022) PDF Code Adapt-and-distill: Developing small, fast and effective pretrained language models for domains. Yunzhi Yao, Shaohan Huang, Wenhui Wang, Li Dong, Furu Wei. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 (ACL 2021) PDF Code">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:published_time" content="2025-02-17T14:25:12-08:00">
    <meta property="article:modified_time" content="2025-02-17T14:25:12-08:00">


    
    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Publication">
  <meta name="twitter:description" content="(*: Equal contribution). See full list in Google Scholar
2025 Reflection on Knowledge Editing: Charting the Next Steps. Yunzhi Yao, Canyu Chen, Jia-Chen Gu, Shumin Deng, Manling Li, Nanyun Peng. Blog CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners. Yunzhi Yao, Jizhan Fang, Jia-Chen Gu, Ningyu Zhang, Shumin Deng, Huajun Chen, Nanyun Peng. PDF Code How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training. Yixin Ou, Yunzhi Yao, Ningyu Zhang, Hui Jin, Jiacheng Sun, Shumin Deng, Zhenguo Li, Huajun Chen. PDF Code 2024 Knowledge Circuits in Pretrained Transformers. Yunzhi Yao, Ningyu Zhang, Zekun Xi, Mengru Wang, Ziwen Xu, Shumin Deng, Huajun Chen. In Proceedings of the 38th Neural Information Processing Systems (Neurips 2024) PDF Code Video Exploring Model Kinship for Merging Large Language Models. Yedi Hu, Yunzhi Yao, Ningyu Zhang, Shumin Deng, Huajun Chen. PDF Code Benchmarking Chinese Knowledge Rectification in Large Language Models. Jizhan Fang*, Tianhe Lu*, Yunzhi Yao*, Xin Xu, Ningyu Zhang, Huajun Chen. PDF Code Knowledge mechanisms in large language models: A survey and perspective. Mengru Wang*, Yunzhi Yao*, Ziwen Xu, Shuofei Qiao, Shumin Deng, Peng Wang, Xiang Chen, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen. In Findings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP 2024) PDF A comprehensive study of knowledge editing for large language models. Ningyu Zhang*, Yunzhi Yao*, Bozhong Tian*, Peng Wang*, Shumin Deng*, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, Huajun Chen. PDF Post 2023 Unveiling the pitfalls of knowledge editing for large language models. Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen. In Proceedings of the 12th International Conference on Learning Representations (ICLR 2023) PDF Code Editing Large Language Models: Problems, Methods, and Opportunities. Yunzhi Yao*, Peng Wang*, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, Ningyu Zhang. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023) PDF Code Video Knowledge Rumination for Pre-trained Language Models. Yunzhi Yao, Peng Wang, Shengyu Mao, Chuanqi Tan, Fei Huang, Huajun Chen, Ningyu Zhang. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023) PDF Code Schema-aware reference as prompt improves data-efficient knowledge graph construction. Yunzhi Yao*, Shengyu Mao*, Ningyu Zhang, Xiang Chen, Shumin Deng, Xi Chen, Huajun Chen. In Proceedings of the 46th International ACM SIGIR Conference (SIGIR 2023) PDF Code Before 2022 Kformer: Knowledge injection in transformer feed-forward layers. Yunzhi Yao, Shaohan Huang, Li Dong, Furu Wei, Huajun Chen, Ningyu Zhang. In Proceedings of the 11th Natural Language Processing and Chinese Computing (NLPCC 2022) PDF Code Adapt-and-distill: Developing small, fast and effective pretrained language models for domains. Yunzhi Yao, Shaohan Huang, Wenhui Wang, Li Dong, Furu Wei. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 (ACL 2021) PDF Code">


    

    <link rel="canonical" href="http://localhost:1313/publication/">
    <link href="/style.min.8a71231d3ff995782d0689340b87c693e7d3cd7c97d5d271f85db466e5714c49.css" rel="stylesheet">
    <link href="/code-highlight.min.706d31975fec544a864cb7f0d847a73ea55ca1df91bf495fd12a177138d807cf.css" rel="stylesheet">

    
    <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png">
    <link rel="mask-icon" href="/icons/safari-pinned-tab.svg">
    <link rel="shortcut icon" href="/favicon.ico">




<link rel="manifest" href="http://localhost:1313/site.webmanifest">

<meta name="msapplication-config" content="/browserconfig.xml">
<meta name="msapplication-TileColor" content="#2d89ef">
<meta name="theme-color" content="#434648">

    
    <link rel="icon" type="image/svg+xml" href="/icons/favicon.svg">

    
    
</head>
<body data-theme = "light" class="notransition">

<script src="/js/theme.js"></script>

<div class="navbar" role="navigation">
    <nav class="menu" aria-label="Main Navigation">
        <a href="http://localhost:1313/" class="logo">
            <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" 
viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" 
stroke-linejoin="round" class="feather feather-home">
<title>Home</title>
<path d="M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"></path>
<polyline points="9 22 9 12 15 12 15 22"></polyline>
</svg>
        </a>
        <input type="checkbox" id="menu-trigger" class="menu-trigger" />
        <label for="menu-trigger">
            <span class="menu-icon">
                <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" stroke="currentColor" fill="none" viewBox="0 0 14 14"><title>Menu</title><path stroke-linecap="round" stroke-linejoin="round" d="M10.595 7L3.40726 7"></path><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 3.51488L3.49301 3.51488"></path><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 10.4851H3.49301"></path><path stroke-linecap="round" stroke-linejoin="round" d="M0.5 12.5V1.5C0.5 0.947715 0.947715 0.5 1.5 0.5H12.5C13.0523 0.5 13.5 0.947715 13.5 1.5V12.5C13.5 13.0523 13.0523 13.5 12.5 13.5H1.5C0.947715 13.5 0.5 13.0523 0.5 12.5Z"></path></svg>
            </span>
        </label>

        <div class="trigger">
            <ul class="trigger-container">
                
                
                <li>
                    <a class="menu-link " href="/">
                        Home
                    </a>
                    
                </li>
                
                <li>
                    <a class="menu-link active" href="/publication/">
                        Publications
                    </a>
                    
                </li>
                
                <li class="menu-separator">
                    <span>|</span>
                </li>
                
                
            </ul>
            <a id="mode" href="#">
                <svg xmlns="http://www.w3.org/2000/svg" class="mode-sunny" width="21" height="21" viewBox="0 0 14 14" stroke-width="1">
<title>LIGHT</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></circle><line x1="7" y1="0.5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="0.5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line></g></svg>
                <svg xmlns="http://www.w3.org/2000/svg" class="mode-moon" width="21" height="21" viewBox="0 0 14 14" stroke-width="1">
<title>DARK</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></circle><line x1="7" y1="0.5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="0.5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line></g></svg>
            </a>
        </div>
    </nav>
</div>

<div class="wrapper post">
    <main class="page-content" aria-label="Content">
        <article>
            <header class="header">
                <h1 class="header-title">Publication</h1>
                
                
                
                
                
            </header>
            
            <div class="page-content">
                <p>(*: Equal contribution). See full list in <a target="_blank" href="https://scholar.google.com/citations?user=nAagIwEAAAAJ">Google Scholar</a></p>
<h3 id="2025">2025</h3>
<ul>
<li>
<article class="pub-item">
  <div>
  <h4 class="pub-item-title">Reflection on Knowledge Editing: Charting the Next Steps.</h4>
  <div><b>Yunzhi Yao</b>, Canyu Chen, Jia-Chen Gu, Shumin Deng, Manling Li, Nanyun Peng.</div>
  <div>
  <span class="pub-item-url"><a class="pub-item-button" target="_blank" href="https://fish-sorrel-a54.notion.site/Reflection-on-Knowledge-Editing-Charting-the-Next-Steps-1e6ca8e41f3a8098bd14c85ac1db8da6">Blog</a></span>
  </article>
</li>
<li>
<article class="pub-item">
  <div>
  <h4 class="pub-item-title">CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners.</h4>
  <div><b>Yunzhi Yao</b>, Jizhan Fang, Jia-Chen Gu, Ningyu Zhang, Shumin Deng, Huajun Chen, Nanyun Peng.</div>
  <div>
  <span class="pub-item-url"><a class="pub-item-button" target="_blank" href="https://arxiv.org/pdf/2503.16356">PDF</a></span>
  <span class="pub-item-url"><a class="pub-item-button" target="_blank" href="https://github.com/zjunlp/CaKE">Code</a></span>
  </article>
</li>
<li>
<article class="pub-item">
  <div>
  <h4 class="pub-item-title">How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training.</h4>
  <div>Yixin Ou, <b>Yunzhi Yao</b>, Ningyu Zhang, Hui Jin, Jiacheng Sun, Shumin Deng, Zhenguo Li, Huajun Chen.</div>
  <div>
  <span class="pub-item-url"><a class="pub-item-button" target="_blank" href="https://arxiv.org/pdf/2502.11196">PDF</a></span>
  <span class="pub-item-url"><a class="pub-item-button" target="_blank" href="https://github.com/zjunlp/DynamicKnowledgeCircuits">Code</a></span>
  </article>
</li>
</ul>
<h3 id="2024">2024</h3>
<ul>
<li>
<article class="pub-item">
  <div>
  <h4 class="pub-item-title">Knowledge Circuits in Pretrained Transformers.</h4>
  <div><b>Yunzhi Yao</b>, Ningyu Zhang, Zekun Xi, Mengru Wang, Ziwen Xu, Shumin Deng, Huajun Chen.</div>
  <div>In Proceedings of the 38th Neural Information Processing Systems (<b>Neurips 2024</b>)</div>
  <div>
  <span class="pub-item-url"><a class="pub-item-button" target="_blank" href="https://arxiv.org/pdf/2405.17969">PDF</a></span>
  <span class="pub-item-url"><a class="pub-item-button" target="_blank" href="https://github.com/zjunlp/KnowledgeCircuits">Code</a></span>
  <span class="pub-item-url"><a class="pub-item-button" target="_blank" href="https://www.youtube.com/watch?v=qDgCLeDs4Kg">Video</a></span>
  </article>
</li>
<li>
<article class="pub-item">
  <div>
  <h4 class="pub-item-title">Exploring Model Kinship for Merging Large Language Models.</h4>
  <div>Yedi Hu, <b>Yunzhi Yao</b>, Ningyu Zhang, Shumin Deng, Huajun Chen.</div>
  <div>
  <span class="pub-item-url"><a class="pub-item-button" target="_blank" href="https://arxiv.org/pdf/2410.12613">PDF</a></span>
  <span class="pub-item-url"><a class="pub-item-button" target="_blank" href="https://github.com/zjunlp/ModelKinship">Code</a></span>
  </article>
</li>
<li>
<article class="pub-item">
  <div>
  <h4 class="pub-item-title">Benchmarking Chinese Knowledge Rectification in Large Language Models.</h4>
  <div>Jizhan Fang*, Tianhe Lu*, <b>Yunzhi Yao*</b>, Xin Xu, Ningyu Zhang, Huajun Chen.</div>
  <div>
  <span class="pub-item-url"><a class="pub-item-button" target="_blank" href="https://arxiv.org/pdf/2409.05806">PDF</a></span>
  <span class="pub-item-url"><a class="pub-item-button" target="_blank" href="https://github.com/zjunlp/EasyEdit">Code</a></span>
  </article>
</li>
<li>
<article class="pub-item">
  <div>
  <h4 class="pub-item-title">Knowledge mechanisms in large language models: A survey and perspective.</h4>
  <div>Mengru Wang*, <b>Yunzhi Yao*</b>, Ziwen Xu, Shuofei Qiao, Shumin Deng, Peng Wang, Xiang Chen, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen.</div>
  <div>In Findings of the 2024 Conference on Empirical Methods in Natural Language Processing (<b>EMNLP 2024</b>)</div>
  <div>
  <span class="pub-item-url"><a class="pub-item-button" target="_blank" href="https://arxiv.org/pdf/2407.15017">PDF</a></span>
  </article>  
</li>
<li>
<article class="pub-item">
  <div>
  <h4 class="pub-item-title">A comprehensive study of knowledge editing for large language models.</h4>
  <div>Ningyu Zhang*, <b>Yunzhi Yao*</b>, Bozhong Tian*, Peng Wang*, Shumin Deng*, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, Huajun Chen.</div>
  <div>
  <span class="pub-item-url"><a class="pub-item-button" target="_blank" href="https://arxiv.org/pdf/2401.01286">PDF</a></span>
  <span class="pub-item-url"><a class="pub-item-button" target="_blank" href="https://x.com/_akhaliq/status/1742371655765164133">Post</a></span>
  </article>  
</li>
</ul>
<h3 id="2023">2023</h3>
<ul>
<li>
<article class="pub-item">
  <div>
  <h4 class="pub-item-title">Unveiling the pitfalls of knowledge editing for large language models.</h4>
  <div>Zhoubo Li, Ningyu Zhang, <b>Yunzhi Yao</b>, Mengru Wang, Xi Chen, Huajun Chen.</div>
  <div>In Proceedings of the 12th International Conference on Learning Representations (<b>ICLR 2023</b>)</div>
  <div>
  <span class="pub-item-url"><a class="pub-item-button" target="_blank" href="https://arxiv.org/pdf/2310.02129">PDF</a></span>
  <span class="pub-item-url"><a class="pub-item-button" target="_blank" href="https://github.com/zjunlp/PitfallsKnowledgeEditing">Code</a></span>
  </article>
</li>
<li>
<article class="pub-item">
  <div>
  <h4 class="pub-item-title">Editing Large Language Models: Problems, Methods, and Opportunities.</h4>
  <div><b>Yunzhi Yao*</b>, Peng Wang*, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, Ningyu Zhang.</div>
  <div>In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (<b>EMNLP 2023</b>)</div>
  <div>
  <span class="pub-item-url"><a class="pub-item-button" target="_blank" href="https://arxiv.org/pdf/2305.13172">PDF</a></span>
  <span class="pub-item-url"><a class="pub-item-button" target="_blank" href="https://github.com/zjunlp/EasyEdit">Code</a></span>
  <span class="pub-item-url"><a class="pub-item-button" target="_blank" href="https://www.bilibili.com/video/BV1MH4y1S737">Video</a></span>
  </article>
</li>
<li>
<article class="pub-item">
  <div>
  <h4 class="pub-item-title">Knowledge Rumination for Pre-trained Language Models.</h4>
  <div><b>Yunzhi Yao</b>, Peng Wang, Shengyu Mao, Chuanqi Tan, Fei Huang, Huajun Chen, Ningyu Zhang.</div>
  <div>In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (<b>EMNLP 2023</b>)</div>
  <div>
  <span class="pub-item-url"><a class="pub-item-button" target="_blank" href="https://arxiv.org/pdf/2305.08732">PDF</a></span>
  <span class="pub-item-url"><a class="pub-item-button" target="_blank" href="https://github.com/zjunlp/knowledge-rumination">Code</a></span>
  </article>
</li>
<li>
<article class="pub-item">
  <div>
  <h4 class="pub-item-title">Schema-aware reference as prompt improves data-efficient knowledge graph construction.</h4>
  <div><b>Yunzhi Yao*</b>, Shengyu Mao*, Ningyu Zhang, Xiang Chen, Shumin Deng, Xi Chen, Huajun Chen.</div>
  <div>In Proceedings of the 46th International ACM SIGIR Conference (<b>SIGIR 2023</b>)</div>
  <div>
  <span class="pub-item-url"><a class="pub-item-button" target="_blank" href="https://arxiv.org/pdf/2210.10709">PDF</a></span>
  <span class="pub-item-url"><a class="pub-item-button" target="_blank" href="https://github.com/zjunlp/RAP">Code</a></span>
  </article>
</li>
</ul>
<h3 id="before-2022">Before 2022</h3>
<ul>
<li>
<article class="pub-item">
  <div>
  <h4 class="pub-item-title">Kformer: Knowledge injection in transformer feed-forward layers.</h4>
  <div><b>Yunzhi Yao</b>, Shaohan Huang, Li Dong, Furu Wei, Huajun Chen, Ningyu Zhang.</div>
  <div>In Proceedings of the 11th Natural Language Processing and Chinese Computing (<b>NLPCC 2022</b>)</div>
  <div>
  <span class="pub-item-url"><a class="pub-item-button" target="_blank" href="https://arxiv.org/pdf/2201.05742">PDF</a></span>
  <span class="pub-item-url"><a class="pub-item-button" target="_blank" href="https://github.com/zjunlp/Kformer">Code</a></span>
  </article>
</li>
<li>
<article class="pub-item">
  <div>
  <h4 class="pub-item-title">Adapt-and-distill: Developing small, fast and effective pretrained language models for domains.</h4>
  <div><b>Yunzhi Yao</b>, Shaohan Huang, Wenhui Wang, Li Dong, Furu Wei.</div>
  <div>In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 (<b>ACL 2021</b>)</div>
  <div>
  <span class="pub-item-url"><a class="pub-item-button" target="_blank" href="https://arxiv.org/pdf/2106.13474">PDF</a></span>
  <span class="pub-item-url"><a class="pub-item-button" target="_blank" href="https://github.com/microsoft/unilm/tree/master/adalm">Code</a></span>
  </article>
</li>
</ul>
<style> 
        .pub-item {
            display: flex;
            margin-top: 0;
            padding-top: 0px;
            padding-bottom: 6px;
            justify-content: space-between;
            flex-direction: row;
            align-items: center;
            line-height: 160%
        }
        .pub-item .pub-item-url {
            color: #4183c4;
            font-size: 14px
        }
        .pub-item .pub-item-url a {
            color: #4183c4;
            transition: all 150ms ease-in 0s
        }
        .pub-item .pub-item-url a:hover {
            background-color: #4183c4;
            color: #fff
        }
        .pub-item .pub-item-button {
            margin-left: 0;
            margin-right: 6px;
            padding-top: 2px;
            padding-bottom: 2px;
            padding-left: 5px;
            padding-right: 5px;
            border: 1px #4183c4 solid;
            border-radius: 4px;
            text-decoration: none
        }
        .pub-item .pub-item-title {
            margin: 0;
            border: 0;
            padding: 0;
            font-size: 16px;
            font-weight: 700;
            color: #4183c4
        }
        .pub-item .pub-item-title a {
            color: #4183c4;
            transition: all 150ms ease-in 0s
        }
    </style>
            </div>
        </article></main>
</div>
<footer class="footer">
    <span class="footer_item"> </span>
    &nbsp;

    
    <small class="footer_copyright">
        © 2025 Yunzhi .
        Powered by <a href="https://github.com/hugo-sid/hugo-blog-awesome" target="_blank" rel="noopener">Hugo blog awesome</a>.
    </small>
</footer><a href="#" title="Go to top" id="totop">
    <svg xmlns="http://www.w3.org/2000/svg" width="50" height="50" fill="currentColor" stroke="currentColor" viewBox="0 96 960 960">
    <path d="M283 704.739 234.261 656 480 410.261 725.739 656 677 704.739l-197-197-197 197Z"/>
</svg>

</a>


    






    
    <script async src="http://localhost:1313/js/main.js" ></script>

    

</body>
</html>
